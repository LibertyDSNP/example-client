"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.getBloomFiltersFor = exports.siftAllByteOffsets = exports.parseBloomFilterOffsets = void 0;
const util_1 = __importDefault(require("../util"));
const parquet_types_1 = __importDefault(require("../../gen-nodejs/parquet_types"));
const sbbf_1 = __importDefault(require("../bloom/sbbf"));
const filterColumnChunksWithBloomFilters = (columnChunkDataCollection) => {
    return columnChunkDataCollection.filter((columnChunk) => {
        const { column: { meta_data: { bloom_filter_offset: { buffer: bloomFilterOffsetBuffer }, }, }, } = columnChunk;
        return bloomFilterOffsetBuffer;
    });
};
const toInteger = (buffer) => {
    const integer = parseInt(buffer.toString("hex"), 16);
    if (integer >= Number.MAX_VALUE) {
        throw Error("Number exceeds Number.MAX_VALUE: Godspeed");
    }
    return integer;
};
const parseBloomFilterOffsets = (ColumnChunkDataCollection) => {
    return ColumnChunkDataCollection.map((columnChunkData) => {
        const { column: { meta_data: { bloom_filter_offset: { buffer: bloomFilterOffsetBuffer }, path_in_schema: pathInSchema, }, }, rowGroupIndex, } = columnChunkData;
        return {
            offsetBytes: toInteger(bloomFilterOffsetBuffer),
            columnName: pathInSchema.join(","),
            rowGroupIndex,
        };
    });
};
exports.parseBloomFilterOffsets = parseBloomFilterOffsets;
const getBloomFilterHeader = async (offsetBytes, envelopeReader) => {
    const headerByteSizeEstimate = 200;
    let bloomFilterHeaderData;
    try {
        bloomFilterHeaderData = await envelopeReader.read(offsetBytes, headerByteSizeEstimate);
    }
    catch (e) {
        throw new Error(e);
    }
    const bloomFilterHeader = new parquet_types_1.default.BloomFilterHeader();
    const sizeOfBloomFilterHeader = util_1.default.decodeThrift(bloomFilterHeader, bloomFilterHeaderData);
    return {
        bloomFilterHeader,
        sizeOfBloomFilterHeader,
    };
};
const readFilterData = async (offsetBytes, envelopeReader) => {
    const { bloomFilterHeader, sizeOfBloomFilterHeader, } = await getBloomFilterHeader(offsetBytes, envelopeReader);
    const { numBytes: filterByteSize } = bloomFilterHeader;
    try {
        const filterBlocksOffset = offsetBytes + sizeOfBloomFilterHeader;
        const buffer = await envelopeReader.read(filterBlocksOffset, filterByteSize);
        return buffer;
    }
    catch (e) {
        throw new Error(e);
    }
};
const readFilterDataFrom = (offsets, envelopeReader) => {
    return Promise.all(offsets.map((offset) => readFilterData(offset, envelopeReader)));
};
const siftAllByteOffsets = (columnChunkDataCollection) => {
    return exports.parseBloomFilterOffsets(filterColumnChunksWithBloomFilters(columnChunkDataCollection));
};
exports.siftAllByteOffsets = siftAllByteOffsets;
const getBloomFiltersFor = async (columnNames, envelopeReader) => {
    const columnChunkDataCollection = envelopeReader.getAllColumnChunkDataFor(columnNames);
    const bloomFilterOffsetData = exports.siftAllByteOffsets(columnChunkDataCollection);
    const offsetByteValues = bloomFilterOffsetData.map(({ offsetBytes }) => offsetBytes);
    const filterBlocksBuffers = await readFilterDataFrom(offsetByteValues, envelopeReader);
    return filterBlocksBuffers.map((buffer, index) => {
        const { columnName, rowGroupIndex } = bloomFilterOffsetData[index];
        return {
            sbbf: sbbf_1.default.from(buffer),
            columnName,
            rowGroupIndex,
        };
    });
};
exports.getBloomFiltersFor = getBloomFiltersFor;
//# sourceMappingURL=bloomFilterReader.js.map